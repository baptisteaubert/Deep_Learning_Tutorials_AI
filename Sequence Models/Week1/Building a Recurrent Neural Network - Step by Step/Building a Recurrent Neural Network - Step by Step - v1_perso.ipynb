{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Recurrent Neural Network - Step by Step\n",
    "\n",
    "Welcome to Course 5's first assignment! In this assignment, you will implement your first Recurrent Neural Network in numpy.\n",
    "\n",
    "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. \n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
    "\n",
    "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
    "    - Example: $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.\n",
    "    \n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.\n",
    "\n",
    "We assume that you are already familiar with `numpy` and/or have completed the previous courses of the specialization. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cell_forward(xt,a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single step of RNN\n",
    "    \n",
    "    Arguments:\n",
    "        xt : word at time t shape(n_x,m)\n",
    "        a_prev: activation of the previous layer shape(n_a,m)\n",
    "        parameters disctionary :\n",
    "                Wax : la matrice qui multiplie x \n",
    "                Waa , ba , Wya , by\n",
    "                \n",
    "    Return: \n",
    "        yt shape (n_y=n_x, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    Waa = parameters['Waa']\n",
    "    Wax = parameters['Wax']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    \n",
    "    yt_pred = softmax(np.dot(at, Wya) + by)\n",
    "    \n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    return a_next, Yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Py3_Deep_Learning",
   "language": "python",
   "name": "py3_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
