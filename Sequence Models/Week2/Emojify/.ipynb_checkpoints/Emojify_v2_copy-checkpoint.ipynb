{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify! \n",
    "\n",
    "Welcome to the second assignment of Week 2. You are going to use word vector representations to build an Emojifier. \n",
    "\n",
    "Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing \"Congratulations on the promotion! Lets get coffee and talk. Love you!\" the emojifier can automatically turn this into \"Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
    "\n",
    "You will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è). In many emoji interfaces, you need to remember that ‚ù§Ô∏è is the \"heart\" symbol rather than the \"love\" symbol. But using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. \n",
    "\n",
    "In this exercise, you'll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. \n",
    "\n",
    "Lets get started! Run the following cell to load the package you are going to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Baseline model: Emojifier-V1\n",
    "\n",
    "### 1.1 - Dataset EMOJISET\n",
    "\n",
    "Let's start by building a simple baseline classifier. \n",
    "\n",
    "You have a tiny dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "Let's load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to print sentences from X_train and corresponding labels from Y_train. Change `index` to see different examples. Because of the font the iPython notebook uses, the heart emoji may be colored black rather than red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am proud of your achievements üòÑ\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the Emojifier-V1\n",
    "\n",
    "In this part, you are going to implement a baseline model called \"Emojifier-v1\".  \n",
    "\n",
    "<center>\n",
    "<img src=\"images/image_1.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Baseline model (Emojifier-V1).</center></caption>\n",
    "</center>\n",
    "\n",
    "The input of the model is a string corresponding to a sentence (e.g. \"I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape  current shape $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, `Y_oh` stands for \"Y-one-hot\" in the variable names `Y_oh_train` and `Y_oh_test`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `convert_to_one_hot()` did. Feel free to change `index` to print out different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Emojifier-V2: Using LSTMs in Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baptisteaubert/Environments/Py3_Deep_Learning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentences_to_indices\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "            \n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, maxLen))\n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        for j in range(len(sentence_words)):\n",
    "            X_indices[i,j] = word_to_index[sentence_words[j]]\n",
    "            \n",
    "    return X_indices\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.]\n",
      " [220930. 286375.  69714.      0.      0.      0.      0.      0.      0.\n",
      "       0.]\n",
      " [151204. 192973. 302254. 151349. 394475.      0.      0.      0.      0.\n",
      "       0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['father'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map['father'].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index,:] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])   #ERROR\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    sentence_to_indice = Input(shape=input_shape, dtype=np.int32)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    Embedding = embedding_layer(sentence_to_indice)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(Embedding)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = Dense(5,activation='softmax')(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(input=sentence_to_indice,output=X)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baptisteaubert/Environments/Py3_Deep_Learning/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 20,223,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 4s 32ms/step - loss: 1.6027 - acc: 0.2348\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 1.5787 - acc: 0.2955\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 1.5630 - acc: 0.2955\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.5511 - acc: 0.3182\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.5423 - acc: 0.3712\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.5184 - acc: 0.3333\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.4768 - acc: 0.4621\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.4670 - acc: 0.5000\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.4267 - acc: 0.5076\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.3378 - acc: 0.6667\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.2628 - acc: 0.6591\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.1940 - acc: 0.7500\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.1321 - acc: 0.7955\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.1109 - acc: 0.8030\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.1047 - acc: 0.8258\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 1.0367 - acc: 0.8788\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0418 - acc: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0130 - acc: 0.8939\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0146 - acc: 0.9015\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9974 - acc: 0.9015\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0370 - acc: 0.8712\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0142 - acc: 0.8939\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 0.9871 - acc: 0.9318\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9664 - acc: 0.9470\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 1.0063 - acc: 0.9015\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9738 - acc: 0.9394\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9568 - acc: 0.9545\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9711 - acc: 0.9318\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9519 - acc: 0.9545\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9509 - acc: 0.9621\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 0.9360 - acc: 0.9697\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9361 - acc: 0.9697\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9353 - acc: 0.9697\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 0.9344 - acc: 0.9697\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9345 - acc: 0.9697\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 0.9325 - acc: 0.9697\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9334 - acc: 0.9697\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.9373 - acc: 0.9697\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9295 - acc: 0.9773\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9339 - acc: 0.9773\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.9242 - acc: 0.9848\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.9235 - acc: 0.9848\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9210 - acc: 0.9848\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.9272 - acc: 0.9773\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.9210 - acc: 0.9848\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.9211 - acc: 0.9848\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9279 - acc: 0.9773\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.9204 - acc: 0.9848\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.9211 - acc: 0.9848\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 2s 14ms/step - loss: 0.9205 - acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10df1ea90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "X_train_indices = sentences_to_indices(X_train,  word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 414us/step\n",
      "\n",
      "Test accuracy =  0.8035714370863778\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You have completed this notebook! ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è\n",
    "\n",
    "<font color='blue'>\n",
    "**What you should remember**:\n",
    "- If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set. \n",
    "- Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details:\n",
    "    - To use mini-batches, the sequences need to be padded so that all the examples in a mini-batch have the same length. \n",
    "    - An `Embedding()` layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it's usually not worth trying to train a large pre-trained set of embeddings.   \n",
    "    - `LSTM()` has a flag called `return_sequences` to decide if you would like to return every hidden states or only the last one. \n",
    "    - You can use `Dropout()` right after `LSTM()` to regularize your network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations on finishing this assignment and building an Emojifier. We hope you're happy with what you've accomplished in this notebook! \n",
    "\n",
    "# üòÄüòÄüòÄüòÄüòÄüòÄ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to Alison Darcy and the Woebot team for their advice on the creation of this assignment. Woebot is a chatbot friend that is ready to speak with you 24/7. As part of Woebot's technology, it uses word embeddings to understand the emotions of what you say. You can play with it by going to http://woebot.io\n",
    "\n",
    "<img src=\"images/woebot.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Py3_Deep_Learning",
   "language": "python",
   "name": "py3_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
